{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de Apache Spark: RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook trabajaremos con los RDDs que forma parte del Spark Core.La implementación de Spark Core es un **RDD (Resilient Distributed Dataset)** que es una colección de datos distribuidos en diferentes nodos del clúster que se procesan en paralelo.\n",
    "\n",
    "Utilizaremos la API de PySpark, pero los conceptos aplican por igual a todas las APIs (Scala, R, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización de Spark en Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\HP\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - findspark\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2021.10.8  |       h5b45459_0         176 KB  conda-forge\n",
      "    certifi-2021.10.8          |   py39hcbf5309_1         145 KB  conda-forge\n",
      "    conda-4.11.0               |   py39hcbf5309_0        16.8 MB  conda-forge\n",
      "    findspark-2.0.1            |     pyhd8ed1ab_0           8 KB  conda-forge\n",
      "    openssl-1.1.1l             |       h8ffe710_0         5.7 MB  conda-forge\n",
      "    python_abi-3.9             |           2_cp39           4 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        22.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  findspark          conda-forge/noarch::findspark-2.0.1-pyhd8ed1ab_0\n",
      "  python_abi         conda-forge/win-64::python_abi-3.9-2_cp39\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2021.10.26~ --> conda-forge::ca-certificates-2021.10.8-h5b45459_0\n",
      "  certifi            pkgs/main::certifi-2021.10.8-py39haa9~ --> conda-forge::certifi-2021.10.8-py39hcbf5309_1\n",
      "  conda              pkgs/main::conda-4.11.0-py39haa95532_0 --> conda-forge::conda-4.11.0-py39hcbf5309_0\n",
      "  openssl              pkgs/main::openssl-1.1.1m-h2bbff1b_0 --> conda-forge::openssl-1.1.1l-h8ffe710_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "ca-certificates-2021 | 176 KB    |            |   0% \n",
      "ca-certificates-2021 | 176 KB    | 9          |   9% \n",
      "ca-certificates-2021 | 176 KB    | ########## | 100% \n",
      "\n",
      "conda-4.11.0         | 16.8 MB   |            |   0% \n",
      "conda-4.11.0         | 16.8 MB   | 1          |   2% \n",
      "conda-4.11.0         | 16.8 MB   | ##1        |  21% \n",
      "conda-4.11.0         | 16.8 MB   | #####5     |  55% \n",
      "conda-4.11.0         | 16.8 MB   | ########5  |  86% \n",
      "conda-4.11.0         | 16.8 MB   | ########## | 100% \n",
      "\n",
      "certifi-2021.10.8    | 145 KB    |            |   0% \n",
      "certifi-2021.10.8    | 145 KB    | ########## | 100% \n",
      "certifi-2021.10.8    | 145 KB    | ########## | 100% \n",
      "\n",
      "python_abi-3.9       | 4 KB      |            |   0% \n",
      "python_abi-3.9       | 4 KB      | ########## | 100% \n",
      "\n",
      "openssl-1.1.1l       | 5.7 MB    |            |   0% \n",
      "openssl-1.1.1l       | 5.7 MB    | ####2      |  42% \n",
      "openssl-1.1.1l       | 5.7 MB    | #########6 |  96% \n",
      "openssl-1.1.1l       | 5.7 MB    | ########## | 100% \n",
      "\n",
      "findspark-2.0.1      | 8 KB      |            |   0% \n",
      "findspark-2.0.1      | 8 KB      | ########## | 100% \n",
      "findspark-2.0.1      | 8 KB      | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el SparkSession y el SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('PySpark_training')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un RDD de una colección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = [1,2,3,4,5]\n",
    "\n",
    "num_rdd = sc.parallelize(num)\n",
    "num_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones\n",
    "* Como sabemos, las Transformaciones son de naturaleza perezosa y no se ejecutarán hasta que se ejecute una Acción sobre ellas.\n",
    "* Intentemos comprender las distintas transformaciones disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map\n",
    "* Esto mapeará su entrada a alguna salida basada en la función especificada en la función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_rdd = num_rdd.map(lambda x : x * 2)\n",
    "double_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtro\n",
    "* Para filtrar los datos en función de una determinada condición. Intentemos encontrar los números pares de num_rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_rdd = num_rdd.filter(lambda x : x % 2 == 0)\n",
    "even_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap\n",
    "* Esta función es muy similar a map, pero puede devolver múltiples elementos para cada entrada en el RDD dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_rdd = num_rdd.flatMap(lambda x : range(1,x))\n",
    "flat_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct\n",
    "* Esto devolverá elementos distintos de un RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 12]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([10, 11, 10, 11, 12, 11])\n",
    "dist_rdd = rdd1.distinct()\n",
    "dist_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey\n",
    "* Esta función reduce los pares de valores clave en función de las claves y una función determinada dentro de reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 8)\n",
      "('b', 8)\n",
      "('c', 6)\n"
     ]
    }
   ],
   "source": [
    "pairs = [ (\"a\", 5), (\"b\", 7), (\"c\", 2), (\"a\", 3), (\"b\", 1), (\"c\", 4)]\n",
    "pair_rdd = sc.parallelize(pairs)\n",
    "\n",
    "output = pair_rdd.reduceByKey(lambda x, y : x + y)\n",
    "\n",
    "result = output.collect()\n",
    "print(*result, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey\n",
    "* Esta función es otra función ByKey que puede operar en un par (clave, valor) RDD pero esto solo agrupará los valores basados en las claves. En otras palabras, esto solo realizará el primer paso de reduceByKey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x1cc3ebdd2e0>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x1cc2e1d5100>),\n",
       " ('c', <pyspark.resultiterable.ResultIterable at 0x1cc2f007e50>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_out = pair_rdd.groupByKey()\n",
    "grp_out.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey\n",
    "* Esta función realizará la clasificación en un par (clave, valor) RDD basado en las claves. De forma predeterminada, la clasificación se realizará en orden ascendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 5)\n",
      "('b', 3)\n",
      "('c', 2)\n",
      "('d', 7)\n"
     ]
    }
   ],
   "source": [
    "pairs = [ (\"a\", 5), (\"d\", 7), (\"c\", 2), (\"b\", 3)]\n",
    "raw_rdd = sc.parallelize(pairs)\n",
    "\n",
    "sortkey_rdd = raw_rdd.sortByKey()\n",
    "result = sortkey_rdd.collect()\n",
    "print(*result,sep='\\n')\n",
    "\n",
    "# Para clasificar en orden descendente, pase  “ascending=False”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordenar por\n",
    "* sortBy es una función más generalizada para ordenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b', 3, 9)\n",
      "('a', 5, 10)\n",
      "('c', 2, 11)\n",
      "('d', 7, 12)\n"
     ]
    }
   ],
   "source": [
    "# Create RDD.\n",
    "pairs = [ (\"a\", 5, 10), (\"d\", 7, 12), (\"c\", 2, 11), (\"b\", 3, 9)]\n",
    "raw_rdd = sc.parallelize(pairs)\n",
    "\n",
    "# Let’s try to do the sorting based on the 3rd element of the tuple.\n",
    "sort_out = raw_rdd.sortBy(lambda x : x[2])\n",
    "result = sort_out.collect()\n",
    "print(*result, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acciones\n",
    "\n",
    "* Las acciones son operaciones en RDD que se ejecutan inmediatamente. Mientras que las transformaciones devuelven otro RDD, las acciones devuelven estructuras de datos nativas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count\n",
    "* Esto contará el número de elementos en el RDD dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = sc.parallelize([1,2,3,4,2])\n",
    "num.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first\n",
    "* Esto devolverá el primer elemento del RDD dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect\n",
    "* Esto devolverá todos los elementos para el RDD dado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No debemos utilizar la operación de collect mientras trabajamos con grandes conjuntos de datos**. Porque devolverá todos los datos que se distribuyen entre los diferentes trabajadores dl clúster a un controlador. Todos los datos viajarán a través de la red del trabajador al conductor y también el conductor necesitaría almacenar todos los datos. Esto obstaculizará el rendimiento de su aplicación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take\n",
    "* Esto devolverá el número de elementos especificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19572/2776498128.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'take'"
     ]
    }
   ],
   "source": [
    "num.take(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
